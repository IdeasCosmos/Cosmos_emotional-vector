"""
Production API System: Enterprise-grade emotion-music transformation service
Complete API implementation with security, monitoring, and scalability
"""

from fastapi import FastAPI, HTTPException, Depends, Security, status, Request, BackgroundTasks
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel, Field, validator
from typing import Dict, List, Optional, Any, Union
import numpy as np
import asyncio
import aioredis
import aiocache
from datetime import datetime, timedelta
import hashlib
import jwt
import json
import logging
from pathlib import Path
import time
from contextlib import asynccontextmanager
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import uvicorn
from enum import Enum
import os
from dataclasses import dataclass, asdict
import traceback

# Import core modules
from cosmos_architecture import (
    HierarchicalEmotionEngine, 
    EmotionVector, 
    MusicParameters,
    ProcessingLevel,
    CosmosAPI
)
from unified_data_layer import (
    QuantumDataStore,
    DataQualityValidator,
    IntelligentDataLoader
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# CONFIGURATION & ENVIRONMENT
# ============================================================================

@dataclass
class APIConfig:
    """Centralized API configuration"""
    # Server
    host: str = os.getenv("API_HOST", "0.0.0.0")
    port: int = int(os.getenv("API_PORT", "8000"))
    workers: int = int(os.getenv("API_WORKERS", "4"))
    
    # Security
    secret_key: str = os.getenv("SECRET_KEY", "quantum-emotion-secret-2025")
    algorithm: str = "HS256"
    access_token_expire_minutes: int = 30
    api_key_header: str = "X-API-Key"
    
    # Redis
    redis_url: str = os.getenv("REDIS_URL", "redis://localhost:6379")
    cache_ttl: int = 3600  # 1 hour
    
    # Rate limiting
    rate_limit_requests: int = 100
    rate_limit_window: int = 60  # seconds
    
    # Performance
    max_batch_size: int = 50
    request_timeout: int = 30
    
    # Data paths
    data_dir: Path = Path("data")
    model_dir: Path = Path("models")
    
    # Features
    enable_gpu: bool = os.getenv("ENABLE_GPU", "false").lower() == "true"
    enable_monitoring: bool = True
    enable_cache: bool = True
    
    def __post_init__(self):
        self.data_dir.mkdir(exist_ok=True)
        self.model_dir.mkdir(exist_ok=True)

config = APIConfig()

# ============================================================================
# METRICS & MONITORING
# ============================================================================

class MetricsCollector:
    """Prometheus metrics collection"""
    
    def __init__(self):
        # Counters
        self.request_count = Counter(
            'cosmos_api_requests_total', 
            'Total API requests',
            ['method', 'endpoint', 'status']
        )
        self.error_count = Counter(
            'cosmos_api_errors_total',
            'Total API errors',
            ['error_type']
        )
        
        # Histograms
        self.request_duration = Histogram(
            'cosmos_api_request_duration_seconds',
            'Request duration in seconds',
            ['endpoint']
        )
        self.emotion_processing_time = Histogram(
            'cosmos_emotion_processing_seconds',
            'Emotion processing time'
        )
        
        # Gauges
        self.active_connections = Gauge(
            'cosmos_api_active_connections',
            'Number of active connections'
        )
        self.cache_hit_rate = Gauge(
            'cosmos_cache_hit_rate',
            'Cache hit rate percentage'
        )
        self.memory_usage = Gauge(
            'cosmos_memory_usage_mb',
            'Memory usage in MB'
        )

metrics = MetricsCollector()

# ============================================================================
# REQUEST/RESPONSE MODELS
# ============================================================================

class ProcessingMode(str, Enum):
    """Processing mode options"""
    FAST = "fast"        # Quick, cached results
    BALANCED = "balanced"  # Standard processing
    DETAILED = "detailed"  # Full hierarchical analysis
    STREAMING = "streaming"  # Real-time streaming

class EmotionRequest(BaseModel):
    """Emotion analysis request model"""
    text: Optional[str] = Field(None, min_length=1, max_length=10000)
    audio_data: Optional[List[float]] = Field(None, max_items=441000)  # 10s @ 44.1kHz
    eeg_bands: Optional[Dict[str, float]] = None
    mode: ProcessingMode = ProcessingMode.BALANCED
    options: Optional[Dict[str, Any]] = None
    session_id: Optional[str] = None
    
    @validator('text')
    def validate_text(cls, v):
        if v and len(v.strip()) == 0:
            raise ValueError("Text cannot be empty")
        return v
    
    @validator('eeg_bands')
    def validate_eeg(cls, v):
        if v:
            required_bands = ['delta', 'theta', 'alpha', 'beta', 'gamma']
            for band in required_bands:
                if band not in v:
                    raise ValueError(f"Missing required EEG band: {band}")
        return v

class EmotionResponse(BaseModel):
    """Emotion analysis response model"""
    request_id: str
    timestamp: datetime
    emotion_vector: List[float]
    confidence: float
    music_parameters: MusicParameters
    processing_time_ms: float
    hierarchical_analysis: Optional[Dict] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)

class BatchRequest(BaseModel):
    """Batch processing request"""
    requests: List[EmotionRequest]
    parallel: bool = True
    
    @validator('requests')
    def validate_batch_size(cls, v):
        if len(v) > config.max_batch_size:
            raise ValueError(f"Batch size exceeds maximum of {config.max_batch_size}")
        return v

class HealthResponse(BaseModel):
    """Health check response"""
    status: str
    version: str = "2.0.0"
    uptime_seconds: float
    services: Dict[str, bool]
    metrics: Optional[Dict[str, float]] = None

# ============================================================================
# AUTHENTICATION & SECURITY
# ============================================================================

class SecurityManager:
    """Handle authentication and authorization"""
    
    def __init__(self):
        self.security = HTTPBearer()
        self.secret_key = config.secret_key
        self.algorithm = config.algorithm
        
    def create_access_token(self, data: dict, expires_delta: Optional[timedelta] = None):
        """Create JWT access token"""
        to_encode = data.copy()
        expire = datetime.utcnow() + (expires_delta or timedelta(minutes=15))
        to_encode.update({"exp": expire, "iat": datetime.utcnow()})
        
        encoded_jwt = jwt.encode(to_encode, self.secret_key, algorithm=self.algorithm)
        return encoded_jwt
    
    def verify_token(self, credentials: HTTPAuthorizationCredentials = Security(HTTPBearer())):
        """Verify JWT token"""
        token = credentials.credentials
        
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])
            username: str = payload.get("sub")
            if username is None:
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Invalid authentication credentials"
                )
            return username
        except jwt.ExpiredSignatureError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Token has expired"
            )
        except jwt.JWTError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Could not validate credentials"
            )
    
    def verify_api_key(self, request: Request):
        """Verify API key from header"""
        api_key = request.headers.get(config.api_key_header)
        
        if not api_key:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="API key required"
            )
        
        # In production, check against database
        valid_keys = ["demo-key-2025", "enterprise-key-premium"]
        
        if api_key not in valid_keys:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Invalid API key"
            )
        
        return api_key

security_manager = SecurityManager()

# ============================================================================
# CACHING LAYER
# ============================================================================

class CacheManager:
    """Distributed caching with Redis"""
    
    def __init__(self):
        self.redis = None
        self.local_cache = {}
        self.cache_stats = {"hits": 0, "misses": 0}
        
    async def connect(self):
        """Connect to Redis"""
        try:
            self.redis = await aioredis.create_redis_pool(
                config.redis_url,
                encoding='utf-8'
            )
            logger.info("Connected to Redis cache")
        except Exception as e:
            logger.warning(f"Redis connection failed: {e}. Using local cache only.")
    
    async def disconnect(self):
        """Disconnect from Redis"""
        if self.redis:
            self.redis.close()
            await self.redis.wait_closed()
    
    def _generate_cache_key(self, request: EmotionRequest) -> str:
        """Generate cache key from request"""
        key_data = {
            "text": request.text,
            "mode": request.mode,
            "options": request.options
        }
        key_str = json.dumps(key_data, sort_keys=True)
        return f"cosmos:{hashlib.md5(key_str.encode()).hexdigest()}"
    
    async def get(self, key: str) -> Optional[Dict]:
        """Get from cache"""
        # Try Redis first
        if self.redis:
            try:
                data = await self.redis.get(key)
                if data:
                    self.cache_stats["hits"] += 1
                    return json.loads(data)
            except Exception as e:
                logger.error(f"Redis get error: {e}")
        
        # Fall back to local cache
        if key in self.local_cache:
            self.cache_stats["hits"] += 1
            return self.local_cache[key]
        
        self.cache_stats["misses"] += 1
        return None
    
    async def set(self, key: str, value: Dict, ttl: int = None):
        """Set in cache"""
        ttl = ttl or config.cache_ttl
        
        # Store in Redis
        if self.redis:
            try:
                await self.redis.setex(
                    key, 
                    ttl, 
                    json.dumps(value)
                )
            except Exception as e:
                logger.error(f"Redis set error: {e}")
        
        # Also store locally
        self.local_cache[key] = value
        
        # Cleanup old local cache entries
        if len(self.local_cache) > 1000:
            # Remove oldest 100 entries
            keys_to_remove = list(self.local_cache.keys())[:100]
            for k in keys_to_remove:
                del self.local_cache[k]
    
    def get_hit_rate(self) -> float:
        """Calculate cache hit rate"""
        total = self.cache_stats["hits"] + self.cache_stats["misses"]
        if total == 0:
            return 0.0
        return self.cache_stats["hits"] / total

cache_manager = CacheManager()

# ============================================================================
# RATE LIMITING
# ============================================================================

class RateLimiter:
    """Token bucket rate limiting"""
    
    def __init__(self):
        self.buckets = {}
        self.lock = asyncio.Lock()
    
    async def check_rate_limit(self, client_id: str) -> bool:
        """Check if client has exceeded rate limit"""
        async with self.lock:
            now = time.time()
            
            if client_id not in self.buckets:
                self.buckets[client_id] = {
                    "tokens": config.rate_limit_requests,
                    "last_update": now
                }
                return True
            
            bucket = self.buckets[client_id]
            
            # Refill tokens
            time_passed = now - bucket["last_update"]
            tokens_to_add = time_passed * (config.rate_limit_requests / config.rate_limit_window)
            bucket["tokens"] = min(
                config.rate_limit_requests,
                bucket["tokens"] + tokens_to_add
            )
            bucket["last_update"] = now
            
            # Check if token available
            if bucket["tokens"] >= 1:
                bucket["tokens"] -= 1
                return True
            
            return False

rate_limiter = RateLimiter()

# ============================================================================
# CORE PROCESSING SERVICE
# ============================================================================

class EmotionProcessingService:
    """Main emotion processing service"""
    
    def __init__(self):
        self.cosmos_api = None
        self.data_store = None
        self.validator = DataQualityValidator()
        
    async def initialize(self):
        """Initialize service components"""
        logger.info("Initializing Emotion Processing Service...")
        
        # Initialize COSMOS engine
        self.cosmos_api = CosmosAPI()
        
        # Initialize data store
        self.data_store = QuantumDataStore()
        
        # Load emotion lexicon
        await self.data_store.load_emotion_lexicon()
        
        logger.info("Emotion Processing Service initialized")
    
    async def process_request(self, request: EmotionRequest) -> EmotionResponse:
        """Process single emotion request"""
        start_time = time.time()
        request_id = hashlib.md5(f"{request.text}{time.time()}".encode()).hexdigest()[:12]
        
        try:
            # Check cache if enabled
            if config.enable_cache and request.mode != ProcessingMode.STREAMING:
                cache_key = cache_manager._generate_cache_key(request)
                cached_result = await cache_manager.get(cache_key)
                
                if cached_result:
                    logger.info(f"Cache hit for request {request_id}")
                    return EmotionResponse(**cached_result)
            
            # Process based on mode
            if request.mode == ProcessingMode.FAST:
                result = await self._process_fast(request)
            elif request.mode == ProcessingMode.DETAILED:
                result = await self._process_detailed(request)
            elif request.mode == ProcessingMode.STREAMING:
                result = await self._process_streaming(request)
            else:  # BALANCED
                result = await self._process_balanced(request)
            
            # Create response
            processing_time = (time.time() - start_time) * 1000
            
            response = EmotionResponse(
                request_id=request_id,
                timestamp=datetime.utcnow(),
                emotion_vector=result["emotion_vector"],
                confidence=result["confidence"],
                music_parameters=MusicParameters(**result["music_parameters"]),
                processing_time_ms=processing_time,
                hierarchical_analysis=result.get("hierarchical_analysis"),
                metadata={
                    "mode": request.mode,
                    "cached": False,
                    "version": "2.0.0"
                }
            )
            
            # Cache result
            if config.enable_cache and request.mode != ProcessingMode.STREAMING:
                await cache_manager.set(
                    cache_key,
                    response.dict(),
                    ttl=config.cache_ttl if request.mode == ProcessingMode.FAST else config.cache_ttl // 2
                )
            
            # Update metrics
            with metrics.emotion_processing_time.time():
                pass
            
            return response
            
        except Exception as e:
            logger.error(f"Processing error for {request_id}: {e}")
            logger.error(traceback.format_exc())
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Processing failed: {str(e)}"
            )
    
    async def _process_fast(self, request: EmotionRequest) -> Dict:
        """Fast processing with minimal analysis"""
        # Use simplified analysis
        if request.text:
            # Quick lexicon lookup
            words = request.text.split()
            emotion_vectors = []
            
            for word in words:
                if word in self.data_store.emotion_lexicon:
                    entry = self.data_store.emotion_lexicon[word]
                    emotion_vectors.append(entry.emotion_vector)
            
            if emotion_vectors:
                avg_emotion = np.mean(emotion_vectors, axis=0)
            else:
                avg_emotion = np.zeros(7)
                avg_emotion[6] = 1.0  # Neutral
            
            # Quick music mapping
            music_mapping = self.data_store.get_music_mapping(avg_emotion)
            
            return {
                "emotion_vector": avg_emotion.tolist(),
                "confidence": 0.7,
                "music_parameters": {
                    "tempo_bpm": np.mean(music_mapping.tempo_range),
                    "key": max(music_mapping.key_preferences, key=music_mapping.key_preferences.get),
                    "mode": max(music_mapping.mode_distribution, key=music_mapping.mode_distribution.get),
                    "chord_progression": music_mapping.chord_progressions[0] if music_mapping.chord_progressions else []
                }
            }
        
        return self._default_result()
    
    async def _process_balanced(self, request: EmotionRequest) -> Dict:
        """Standard balanced processing"""
        result = await self.cosmos_api.analyze(
            request.text or "",
            options=request.options
        )
        return result
    
    async def _process_detailed(self, request: EmotionRequest) -> Dict:
        """Detailed hierarchical processing"""
        result = await self.cosmos_api.analyze(
            request.text or "",
            options={**(request.options or {}), "detailed": True}
        )
        return result
    
    async def _process_streaming(self, request: EmotionRequest) -> Dict:
        """Streaming processing for real-time analysis"""
        # This would implement streaming logic
        # For now, return balanced result
        return await self._process_balanced(request)
    
    def _default_result(self) -> Dict:
        """Default neutral result"""
        return {
            "emotion_vector": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
            "confidence": 0.5,
            "music_parameters": {
                "tempo_bpm": 120,
                "key": "C",
                "mode": "major",
                "chord_progression": ["I", "IV", "V", "I"]
            }
        }

# ============================================================================
# API APPLICATION
# ============================================================================

# Lifespan context manager for startup/shutdown
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifecycle"""
    # Startup
    logger.info("Starting COSMOS API...")
    
    # Initialize services
    await cache_manager.connect()
    await processing_service.initialize()
    
    # Start background tasks
    asyncio.create_task(monitor_system_health())
    
    logger.info("COSMOS API started successfully")
    
    yield
    
    # Shutdown
    logger.info("Shutting down COSMOS API...")
    
    await cache_manager.disconnect()
    
    logger.info("COSMOS API shutdown complete")

# Create FastAPI application
app = FastAPI(
    title="COSMOS Emotion-Music API",
    description="Advanced hierarchical emotion to music transformation system",
    version="2.0.0",
    lifespan=lifespan
)

# Add middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

app.add_middleware(GZipMiddleware, minimum_size=1000)

# Initialize services
processing_service = EmotionProcessingService()

# ============================================================================
# API ENDPOINTS
# ============================================================================

@app.get("/", response_model=Dict[str, Any])
async def root():
    """Root endpoint with API information"""
    return {
        "name": "COSMOS Emotion-Music API",
        "version": "2.0.0",
        "status": "operational",
        "documentation": "/docs",
        "endpoints": {
            "process": "/api/v2/process",
            "batch": "/api/v2/batch",
            "stream": "/api/v2/stream",
            "health": "/health",
            "metrics": "/metrics"
        }
    }

@app.post("/api/v2/process", response_model=EmotionResponse)
async def process_emotion(
    request: EmotionRequest,
    background_tasks: BackgroundTasks,
    api_key: str = Depends(security_manager.verify_api_key)
):
    """Process single emotion analysis request"""
    
    # Rate limiting
    client_id = api_key  # In production, use IP or user ID
    if not await rate_limiter.check_rate_limit(client_id):
        raise HTTPException(
            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
            detail="Rate limit exceeded"
        )
    
    # Update metrics
    metrics.request_count.labels(
        method="POST",
        endpoint="/api/v2/process",
        status="processing"
    ).inc()
    
    # Process request
    with metrics.request_duration.labels(endpoint="/api/v2/process").time():
        response = await processing_service.process_request(request)
    
    # Background tasks
    background_tasks.add_task(
        log_request,
        request_id=response.request_id,
        request=request,
        response=response
    )
    
    return response

@app.post("/api/v2/batch", response_model=List[EmotionResponse])
async def process_batch(
    batch: BatchRequest,
    api_key: str = Depends(security_manager.verify_api_key)
):
    """Process batch of emotion analysis requests"""
    
    # Rate limiting (count as multiple requests)
    client_id = api_key
    for _ in range(len(batch.requests)):
        if not await rate_limiter.check_rate_limit(client_id):
            raise HTTPException(
                status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                detail="Rate limit exceeded for batch"
            )
    
    # Process requests
    if batch.parallel:
        # Parallel processing
        tasks = [
            processing_service.process_request(req)
            for req in batch.requests
        ]
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Handle exceptions
        final_responses = []
        for i, response in enumerate(responses):
            if isinstance(response, Exception):
                logger.error(f"Batch item {i} failed: {response}")
                # Return error response
                final_responses.append(
                    EmotionResponse(
                        request_id=f"batch_{i}_error",
                        timestamp=datetime.utcnow(),
                        emotion_vector=[0.0] * 7,
                        confidence=0.0,
                        music_parameters=MusicParameters(),
                        processing_time_ms=0.0,
                        metadata={"error": str(response)}
                    )
                )
            else:
                final_responses.append(response)
        
        return final_responses
    else:
        # Sequential processing
        responses = []
        for req in batch.requests:
            response = await processing_service.process_request(req)
            responses.append(response)
        return responses

@app.get("/api/v2/stream")
async def stream_emotions(
    session_id: Optional[str] = None,
    api_key: str = Depends(security_manager.verify_api_key)
):
    """Stream real-time emotion analysis"""
    
    async def event_generator():
        """Generate server-sent events"""
        try:
            while True:
                # In production, this would receive from a queue
                await asyncio.sleep(1)
                
                # Generate sample emotion
                emotion = np.random.randn(7) * 0.3
                emotion[6] = 0.5  # Some neutrality
                
                event_data = {
                    "timestamp": datetime.utcnow().isoformat(),
                    "emotion_vector": emotion.tolist(),
                    "session_id": session_id or "default"
                }
                
                yield f"data: {json.dumps(event_data)}\n\n"
                
        except asyncio.CancelledError:
            logger.info(f"Stream cancelled for session {session_id}")
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    
    # Check service health
    services_status = {
        "cosmos_engine": processing_service.cosmos_api is not None,
        "data_store": processing_service.data_store is not None,
        "cache": cache_manager.redis is not None or len(cache_manager.local_cache) > 0,
        "metrics": config.enable_monitoring
    }
    
    # Calculate uptime
    uptime = time.time() - app.state.start_time if hasattr(app.state, 'start_time') else 0
    
    # Get metrics
    metrics_data = None
    if config.enable_monitoring:
        metrics_data = {
            "cache_hit_rate": cache_manager.get_hit_rate(),
            "active_connections": metrics.active_connections._value.get(),
            "total_requests": sum(metrics.request_count._metrics.values())
        }
    
    return HealthResponse(
        status="healthy" if all(services_status.values()) else "degraded",
        uptime_seconds=uptime,
        services=services_status,
        metrics=metrics_data
    )

@app.get("/metrics")
async def get_metrics():
    """Prometheus metrics endpoint"""
    
    if not config.enable_monitoring:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Metrics not enabled"
        )
    
    # Update runtime metrics
    metrics.cache_hit_rate.set(cache_manager.get_hit_rate() * 100)
    
    # Get memory usage
    import psutil
    process = psutil.Process()
    memory_mb = process.memory_info().rss / 1024 / 1024
    metrics.memory_usage.set(memory_mb)
    
    return Response(
        content=generate_latest(),
        media_type="text/plain"
    )

@app.post("/auth/token")
async def get_token(username: str, password: str):
    """Get authentication token"""
    
    # In production, verify against database
    if username == "demo" and password == "cosmos2025":
        access_token = security_manager.create_access_token(
            data={"sub": username},
            expires_delta=timedelta(minutes=config.access_token_expire_minutes)
        )
        return {"access_token": access_token, "token_type": "bearer"}
    
    raise HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Invalid credentials"
    )

# ============================================================================
# BACKGROUND TASKS
# ============================================================================

async def log_request(request_id: str, request: EmotionRequest, response: EmotionResponse):
    """Log request for analytics"""
    log_entry = {
        "request_id": request_id,
        "timestamp": response.timestamp.isoformat(),
        "mode": request.mode,
        "processing_time_ms": response.processing_time_ms,
        "confidence": response.confidence,
        "cached": response.metadata.get("cached", False)
    }
    
    # In production, write to database or analytics service
    logger.info(f"Request logged: {json.dumps(log_entry)}")

async def monitor_system_health():
    """Monitor system health in background"""
    while True:
        try:
            # Check memory usage
            import psutil
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            
            if memory_mb > 1000:  # 1GB threshold
                logger.warning(f"High memory usage: {memory_mb:.2f} MB")
            
            # Check cache hit rate
            hit_rate = cache_manager.get_hit_rate()
            if hit_rate < 0.3 and cache_manager.cache_stats["hits"] + cache_manager.cache_stats["misses"] > 100:
                logger.warning(f"Low cache hit rate: {hit_rate:.2%}")
            
            await asyncio.sleep(60)  # Check every minute
            
        except Exception as e:
            logger.error(f"Health monitoring error: {e}")
            await asyncio.sleep(60)

# ============================================================================
# ERROR HANDLERS
# ============================================================================

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """Handle HTTP exceptions"""
    
    metrics.error_count.labels(error_type="http").inc()
    
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "status_code": exc.status_code,
            "timestamp": datetime.utcnow().isoformat()
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """Handle general exceptions"""
    
    metrics.error_count.labels(error_type="general").inc()
    
    logger.error(f"Unhandled exception: {exc}")
    logger.error(traceback.format_exc())
    
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "error": "Internal server error",
            "status_code": 500,
            "timestamp": datetime.utcnow().isoformat()
        }
    )

# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """Run the API server"""
    
    # Set start time
    app.state.start_time = time.time()
    
    # Configure uvicorn
    uvicorn.run(
        app,
        host=config.host,
        port=config.port,
        workers=config.workers,
        log_level="info",
        access_log=True,
        use_colors=True,
        reload=False  # Set to True for development
    )

if __name__ == "__main__":
    main()